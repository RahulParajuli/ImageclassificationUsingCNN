# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d7UXjn1lAUutSP_MFOlKi-RZlje89V4W

<h1>Accident detection using CNN image classification</h1>

<h2>Mouting Google drive</h2>
"""

from google.colab import drive
drive.mount('/content/drive')

"""Checking avaiable GPU"""



"""Confirming the prescence of GPU"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""Setting parameters for the model"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt


# %matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'
plt.show()

# %load_ext autoreload
# %autoreload 2

np.random.seed(1)

"""Importing all the necessary libraries"""

import cv2
import os
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
import warnings
warnings.filterwarnings("ignore")

from tensorflow import keras
from tensorflow.keras import layers
from tensorflow.keras.models import Sequential
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array

"""Checking directories"""

os.listdir('/content/drive/MyDrive/dataforai/train')

os.listdir('/content/drive/MyDrive/dataforai/test')

"""Assigning variables for datasets<br>
train_dir = train directory of the dataset<br>
test_dir = test directory of the dataset
"""

train_dir = "/content/drive/MyDrive/dataforai/train/"
test_dir  = "/content/drive/MyDrive/dataforai/test/"
categories = ['Accident','Noaccident']
train_dir
# test_dir

"""Initializing batch_size, image height and width"""

batch_size = 32
img_height = 256
img_width = 256

"""Preprocessing all the Image data from test dataset<br>
Using label as inferred since the dataset is binary class classification<br>
Color channel of RGB<br>

"""

val_ds = tf.keras.utils.image_dataset_from_directory(
    test_dir,
    labels='inferred',
    class_names=None,
    color_mode='rgb',
    batch_size=32,
    image_size=(256, 256),
    shuffle=True,
    seed=None,
    validation_split=None,
    crop_to_aspect_ratio=True,
  )

"""
Preprocessing all the Image data from train dataset<br>
Using label as inferred since the dataset is binary class classification<br>
Color channel of RGB<br>
"""

train_ds = tf.keras.utils.image_dataset_from_directory(
   train_dir,
   labels = 'inferred',
   class_names = None,
   color_mode = 'rgb',
   batch_size=32,
    image_size=(256, 256),
    shuffle=True,
    seed=None,
    validation_split=None,
    crop_to_aspect_ratio=True,
)

"""<h4> Total images to process </h4><br>
Training image = 1302 belonging to 2 classes i.e accident and no accident <br>
Test/validation image = 382 belonging to 2 classes i.e accident and no accident
"""

class_names = train_ds.class_names
print(class_names)

"""Lets plot some of the images from dataset with their labels"""

plt.figure(figsize=(10, 10))
for images, labels in train_ds.take(1):
  for i in range(9):
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(images[i].numpy().astype("uint8"))
    plt.title(class_names[labels[i]])
    plt.axis("off")

for image_batch, labels_batch in train_ds:
  print(image_batch.shape)
  print(labels_batch.shape)
  break

"""<h3>Creating convolution layers</h3> <br>
Rescaling images by ./255 and channelling to 3 (rgb) with image height of 256 and image width of 256 <br>
Conv2D of different hidden layers/ hyperparameter is used along with padding same<br>
Relu is taken for Activation function for the model <br>
Maxpooling for compressing the image<br>
Finally the images are flattened and densed 

"""

num_classes = len(class_names)

model = Sequential([
  layers.Rescaling(1./255, input_shape=(img_height, img_width, 3)),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

"""Here adam is assigned as a optimizer with the learning rate of 1e-3 <br>
Loss as a binary cross entropy
Metrics for evaluation as BinaryAccuracy and falsenegatives
"""

his = model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=tf.keras.losses.BinaryCrossentropy(),
              metrics=[tf.keras.metrics.BinaryAccuracy(),
                       tf.keras.metrics.FalseNegatives()])

"""To evaluate furthermore loss of Sparse Categorical cross entropy is taken<br>
And metrics as Accuracy
"""

his_model = model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

"""Finally the summary of the model is printed"""

model.summary()

"""<h3> Model summary</h3><br>
Use of 3 convolution layers<br> trainable parameters of the model = 8,412,578

------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

<h1> Model training</h1>
"""

epochs=10
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs,
)

"""Training the model with 10 epochs"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""<h3> Findings</h3> <br>
As the model is trained with 10 epoch there is noticable disrepancy in the training and validation ratio.<br>
There is drastic different between training loss and validation loss<br>
Such condition is <b>overfitting condition</b> and our model has over fitted

To reduce <b>overfitting</b> we use <b>data augmentation</b> with <b>dropout</b> to our model
"""

data_augmentation = keras.Sequential(
  [
    layers.RandomFlip("horizontal",
                      input_shape=(img_height,
                                  img_width,
                                  3)),
    layers.RandomRotation(0.1),
    layers.RandomZoom(0.1),
  ]
)

"""Data augmentation rotates the images from training dataset and randomly rotate/zoom for better understanding of the image<br>
An example is plotted below
"""

plt.figure(figsize=(10, 10))
for images, _ in train_ds.take(1):
  for i in range(9):
    augmented_images = data_augmentation(images)
    ax = plt.subplot(3, 3, i + 1)
    plt.imshow(augmented_images[0].numpy().astype("uint8"))
    plt.axis("off")

"""<h2> Training model after data augmentation</h2>"""

model = Sequential([
  data_augmentation,
  layers.Rescaling(1./255),
  layers.Conv2D(16, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(32, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Conv2D(64, 3, padding='same', activation='relu'),
  layers.MaxPooling2D(),
  layers.Dropout(0.2),
  layers.Flatten(),
  layers.Dense(128, activation='relu'),
  layers.Dense(num_classes)
])

model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

model.summary()

"""We have just added dropout to our layer to train from every perspectives of the image <br>
Model is now trained with <b> 15 epochs
"""

epochs = 15
history = model.fit(
  train_ds,
  validation_data=val_ds,
  epochs=epochs
)

"""<h2> Plotting new graph"""

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']

loss = history.history['loss']
val_loss = history.history['val_loss']

epochs_range = range(epochs)

plt.figure(figsize=(8, 8))
plt.subplot(1, 2, 1)
plt.plot(epochs_range, acc, label='Training Accuracy')
plt.plot(epochs_range, val_acc, label='Validation Accuracy')
plt.legend(loc='lower right')
plt.title('Training and Validation Accuracy')

plt.subplot(1, 2, 2)
plt.plot(epochs_range, loss, label='Training Loss')
plt.plot(epochs_range, val_loss, label='Validation Loss')
plt.legend(loc='upper right')
plt.title('Training and Validation Loss')
plt.show()

"""There is difference between the two lines, but it has significantly decreased as the layer receives data augmentation which has reduce the cause of over fitting of the model.

<h1> Testing with images
"""

image = "/content/drive/MyDrive/AccidentDataset/test/accident/11730.png"

img = tf.keras.utils.load_img(
    image, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print( "This image most likely belongs to {} with a {:.2f} percent confidence.".format(class_names[np.argmax(score)], 100 * np.max(score)))
plt.imshow(img)

image = "/content/drive/MyDrive/AccidentDataset/test/noaccident/no10002.png"

img = tf.keras.utils.load_img(
    image, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print( "This image most likely belongs to {} with a {:.2f} percent confidence.".format(class_names[np.argmax(score)], 100 * np.max(score)))
plt.imshow(img)

image = "/content/drive/MyDrive/AccidentDataset/test/noaccident/no10576.png"

img = tf.keras.utils.load_img(
    image, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print( "This image most likely belongs to {} with a {:.2f} percent confidence.".format(class_names[np.argmax(score)], 100 * np.max(score)))
plt.imshow(img)

image = "/content/drive/MyDrive/AccidentDataset/test/noaccident/no10907.png"

img = tf.keras.utils.load_img(
    image, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print( "This image most likely belongs to {} with a {:.2f} percent confidence.".format(class_names[np.argmax(score)], 100 * np.max(score)))
plt.imshow(img)

image = "/content/drive/MyDrive/AccidentDataset/test/accident/12553.png"

img = tf.keras.utils.load_img(
    image, target_size=(img_height, img_width)
)
img_array = tf.keras.utils.img_to_array(img)
img_array = tf.expand_dims(img_array, 0) # Create a batch

predictions = model.predict(img_array)
score = tf.nn.softmax(predictions[0])

print( "This image most likely belongs to {} with a {:.2f} percent confidence.".format(class_names[np.argmax(score)], 100 * np.max(score)))
plt.imshow(img)

"""<h1> Exporting the model"""

import pickle

import pickle
modelname = "accident"
pickle.dump(model,open(modelname,"wb"))

"""<h1> Conclusion

Accident detection is binary class classification of the images which uses CNN for model building and feature extractions. First part of the documention we only used raw data which are only proessed according to the provided data. despite there is limited data to train the model it has showed a good amount of accuracy of 80% these are well tested after the model building. Such system can be used to solve problem of detecting anamolies in the road situations. This project has taught us a gratitude of knowledge to kick start the vision towards learning neural networks.
"""
